name: "MSDF"
layer {
  name: "data"
  type: "ImageLabelmapData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    mean_value: 104.00699
    mean_value: 116.66877
    mean_value: 122.67892
  }
  image_data_param {
    root_folder: "./data/BSDS500/"
    source: "./data/BSDS500/train_pair.lst"
    batch_size: 1
    shuffle: true
    new_height: 0
    new_width: 0
  }
}

##**VGG-16 structure**##

layer { bottom: 'data' top: 'conv1_1' name: 'conv1_1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 64 pad: 35  kernel_size: 3 } }
layer { bottom: 'conv1_1' top: 'conv1_1' name: 'relu1_1' type: "ReLU" }
layer { bottom: 'conv1_1' top: 'conv1_2' name: 'conv1_2' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 64 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv1_2' top: 'conv1_2' name: 'relu1_2' type: "ReLU" }
layer { name: 'pool1' bottom: 'conv1_2' top: 'pool1' type: "Pooling"
  pooling_param { pool: MAX kernel_size: 2 stride: 2 } }

layer { name: 'conv2_1' bottom: 'pool1' top: 'conv2_1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 128 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv2_1' top: 'conv2_1' name: 'relu2_1' type: "ReLU" }
layer { bottom: 'conv2_1' top: 'conv2_2' name: 'conv2_2' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 128 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv2_2' top: 'conv2_2' name: 'relu2_2' type: "ReLU" }
layer { bottom: 'conv2_2' top: 'pool2' name: 'pool2' type: "Pooling"
  pooling_param { pool: MAX kernel_size: 2 stride: 2 } }

layer { bottom: 'pool2' top: 'conv3_1' name: 'conv3_1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 256 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv3_1' top: 'conv3_1' name: 'relu3_1' type: "ReLU" }
layer { bottom: 'conv3_1' top: 'conv3_2' name: 'conv3_2' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 256 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv3_2' top: 'conv3_2' name: 'relu3_2' type: "ReLU" }
layer { bottom: 'conv3_2' top: 'conv3_3' name: 'conv3_3' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 256 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv3_3' top: 'conv3_3' name: 'relu3_3' type: "ReLU" }
layer { bottom: 'conv3_3' top: 'pool3' name: 'pool3' type: "Pooling"
  pooling_param { pool: MAX kernel_size: 2 stride: 2 } }

layer { bottom: 'pool3' top: 'conv4_1' name: 'conv4_1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 512 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv4_1' top: 'conv4_1' name: 'relu4_1' type: "ReLU" }
layer { bottom: 'conv4_1' top: 'conv4_2' name: 'conv4_2' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 512 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv4_2' top: 'conv4_2' name: 'relu4_2' type: "ReLU" }
layer { bottom: 'conv4_2' top: 'conv4_3' name: 'conv4_3' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 512 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv4_3' top: 'conv4_3' name: 'relu4_3' type: "ReLU" }
layer { bottom: 'conv4_3' top: 'pool4' name: 'pool4' type: "Pooling"
  pooling_param { pool: MAX kernel_size: 2 stride: 2 } }

layer { bottom: 'pool4' top: 'conv5_1' name: 'conv5_1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 512 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv5_1' top: 'conv5_1' name: 'relu5_1' type: "ReLU" }
layer { bottom: 'conv5_1' top: 'conv5_2' name: 'conv5_2' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 512 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv5_2' top: 'conv5_2' name: 'relu5_2' type: "ReLU" }
layer { bottom: 'conv5_2' top: 'conv5_3' name: 'conv5_3' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 512 pad: 1 kernel_size: 3 } }
layer { bottom: 'conv5_3' top: 'conv5_3' name: 'relu5_3' type: "ReLU" }


##**MSDF new added**##

layer { bottom: 'conv5_3' top: 'conv5_h' name: 'conv5_h' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 256 pad: 1 kernel_size: 3 weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv5_h' top: 'conv5_h' name: 'relu5_h' type: "ReLU" }

layer {
    bottom: "conv5_h"
    top: "conv5_h_branch1a"
    name: "conv5_h_branch1a"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv5_h_branch1a"
    top: "conv5_h_branch1a"
    name: "bn5_branch1a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv5_h_branch1a"
    top: "conv5_h_branch1a"
    name: "scale5h_branch1a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv5_h_branch1a"
    top: "conv5_h_branch1a"
    name: "conv5_h_branch1a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv5_h_branch1a"
    top: "conv5_h_branch2a"
    name: "conv5_h_branch2a"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv5_h_branch2a"
    top: "conv5_h_branch2a"
    name: "bn5_branch2a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv5_h_branch2a"
    top: "conv5_h_branch2a"
    name: "scale5h_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv5_h_branch2a"
    top: "conv5_h_branch2a"
    name: "conv5_h_branch2a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv5_h_branch2a"
    top: "conv5_h_branch3a"
    name: "conv5_h_branch3a"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv5_h_branch3a"
    top: "conv5_h_branch3a"
    name: "bn5_branch3a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv5_h_branch3a"
    top: "conv5_h_branch3a"
    name: "scale5h_branch3a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv5_h"
    bottom: "conv5_h_branch3a"
    top: "conv5_ha"
    name: "conv5_ha"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "conv5_ha"
    top: "conv5_ha"
    name: "conv5_ha_relu"
    type: "ReLU"
}


layer {name: "conv5_subConv" type: "Convolution" bottom: "conv5_ha" top: "conv5_subConv"
  param {lr_mult: 1 decay_mult: 1} param {lr_mult: 2 decay_mult: 0} convolution_param {num_output: 1024 kernel_size: 3 pad:1 weight_filler{ type: "gaussian" std: 0.01 } bias_filler{ type: "constant" value: 0} } }


layer {
  type: "Python" 
  name: "conv5_up" 
  bottom: "conv5_subConv" 
  top: "conv5_up"   
  python_param {
      module: "PyPixelShuffleLayer"
      layer: "PyPixelShuffleLayer"
      param_str: "'scale_factor': 2"
  } 
}


layer { bottom: 'conv4_3' top: 'conv4_h' name: 'conv4_h' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 256 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv4_h' top: 'conv4_h' name: 'relu4_h' type: "ReLU" }

layer {
    bottom: "conv4_h"
    top: "conv4_h_branch1a"
    name: "conv4_h_branch1a"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv4_h_branch1a"
    top: "conv4_h_branch1a"
    name: "bn4_branch1a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv4_h_branch1a"
    top: "conv4_h_branch1a"
    name: "scale4h_branch1a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv4_h_branch1a"
    top: "conv4_h_branch1a"
    name: "conv4_h_branch1a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv4_h_branch1a"
    top: "conv4_h_branch2a"
    name: "conv4_h_branch2a"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv4_h_branch2a"
    top: "conv4_h_branch2a"
    name: "bn4_branch2a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv4_h_branch2a"
    top: "conv4_h_branch2a"
    name: "scale4h_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv4_h_branch2a"
    top: "conv4_h_branch2a"
    name: "conv4_h_branch2a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv4_h_branch2a"
    top: "conv4_h_branch3a"
    name: "conv4_h_branch3a"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv4_h_branch3a"
    top: "conv4_h_branch3a"
    name: "bn4_branch3a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv4_h_branch3a"
    top: "conv4_h_branch3a"
    name: "scale4h_branch3a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv4_h"
    bottom: "conv4_h_branch3a"
    top: "conv4_ha"
    name: "conv4_ha"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "conv4_ha"
    top: "conv4_ha"
    name: "conv4_ha_relu"
    type: "ReLU"
}



layer { bottom: 'conv5_up' bottom: 'conv4_ha' top: 'up_conv5' name: 'align4' type: "Crop" crop_param { axis: 2} propagate_down: 1 propagate_down: 0 }
layer { bottom: 'conv4_ha' bottom: 'up_conv5' top: 'conv4_r' name: 'concat4' type: 'Concat' concat_param {axis: 1} }

layer { bottom: 'conv4_r' top: 'conv4_d1' name: 'conv4_d1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 128 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv4_d1' top: 'conv4_d1' name: 'relu4_d1' type: "ReLU" }

layer {name: "conv4_subConv" type: "Convolution" bottom: "conv4_d1" top: "conv4_subConv"
  param {lr_mult: 1 decay_mult: 1} param {lr_mult: 2 decay_mult: 0} convolution_param {num_output: 512 kernel_size: 3 pad:1 weight_filler{ type: "gaussian" std: 0.01 } bias_filler{ type: "constant" value: 0} } }


layer {
  type: "Python" 
  name: "conv4_up" 
  bottom: "conv4_subConv" 
  top: "conv4_up"   
  python_param {
      module: "PyPixelShuffleLayer"
      layer: "PyPixelShuffleLayer"
      param_str: "'scale_factor': 2"
  } 
}


layer { bottom: 'conv3_3' top: 'conv3_h' name: 'conv3_h' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 128 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv3_h' top: 'conv3_h' name: 'relu3_h' type: "ReLU" }

layer {
    bottom: "conv3_h"
    top: "conv3h_branch1a"
    name: "conv3h_branch1a"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv3h_branch1a"
    top: "conv3h_branch1a"
    name: "bn3_branch1a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv3h_branch1a"
    top: "conv3h_branch1a"
    name: "scale3h_branch1a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv3h_branch1a"
    top: "conv3h_branch1a"
    name: "conv3h_branch1a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv3h_branch1a"
    top: "conv3h_branch2a"
    name: "conv3h_branch2a"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv3h_branch2a"
    top: "conv3h_branch2a"
    name: "bn3_branch2a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv3h_branch2a"
    top: "conv3h_branch2a"
    name: "scale3h_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv3h_branch2a"
    top: "conv3h_branch2a"
    name: "conv3h_branch2a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv3h_branch2a"
    top: "conv3h_branch3a"
    name: "conv3h_branch3a"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv3h_branch3a"
    top: "conv3h_branch3a"
    name: "bn3_branch3a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv3h_branch3a"
    top: "conv3h_branch3a"
    name: "scale3h_branch3a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv3_h"
    bottom: "conv3h_branch3a"
    top: "conv3_ha"
    name: "conv3_ha"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "conv3_ha"
    top: "conv3_ha"
    name: "conv3_ha_relu"
    type: "ReLU"
}


layer { bottom: 'conv4_up' bottom: 'conv3_ha' top: 'up_conv4' name: 'align3' type: "Crop" crop_param { axis: 2} propagate_down: 1 propagate_down: 0}
layer { bottom: 'conv3_ha' bottom: 'up_conv4' top: 'conv3_r' name: 'concat3' type: 'Concat'
  concat_param {axis: 1} }

layer { bottom: 'conv3_r' top: 'conv3_d1' name: 'conv3_d1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 64 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv3_d1' top: 'conv3_d1' name: 'relu3_d1' type: "ReLU" }


layer {name: "conv3_subConv" type: "Convolution" bottom: "conv3_d1" top: "conv3_subConv"
  param {lr_mult: 1 decay_mult: 1} param {lr_mult: 2 decay_mult: 0} convolution_param {num_output: 256 kernel_size: 3 pad:1 weight_filler{ type: "gaussian" std: 0.01 } bias_filler{ type: "constant" value: 0} } }


layer {
  type: "Python" 
  name: "conv3_up" 
  bottom: "conv3_subConv" 
  top: "conv3_up"   
  python_param {
      module: "PyPixelShuffleLayer"
      layer: "PyPixelShuffleLayer"
      param_str: "'scale_factor': 2"
  } 
}


layer { bottom: 'conv2_2' top: 'conv2_h' name: 'conv2_h' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 64 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv2_h' top: 'conv2_h' name: 'relu2_h' type: "ReLU" }

layer {
    bottom: "conv2_h"
    top: "conv2h_branch1a"
    name: "conv2h_branch1a"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv2h_branch1a"
    top: "conv2h_branch1a"
    name: "bn2_branch1a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv2h_branch1a"
    top: "conv2h_branch1a"
    name: "scale2h_branch1a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv2h_branch1a"
    top: "conv2h_branch1a"
    name: "conv2h_branch1a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv2h_branch1a"
    top: "conv2h_branch2a"
    name: "conv2h_branch2a"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv2h_branch2a"
    top: "conv2h_branch2a"
    name: "bn2_branch2a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv2h_branch2a"
    top: "conv2h_branch2a"
    name: "scale2h_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv2h_branch2a"
    top: "conv2h_branch2a"
    name: "conv2h_branch2a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv2h_branch2a"
    top: "conv2h_branch3a"
    name: "conv2h_branch3a"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv2h_branch3a"
    top: "conv2h_branch3a"
    name: "bn2_branch3a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv2h_branch3a"
    top: "conv2h_branch3a"
    name: "scale2h_branch3a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv2_h"
    bottom: "conv2h_branch3a"
    top: "conv2_ha"
    name: "conv2_ha"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "conv2_ha"
    top: "conv2_ha"
    name: "conv2_ha_relu"
    type: "ReLU"
}




layer { bottom: 'conv3_up' bottom: 'conv2_ha' top: 'up_conv3' name: 'align2' type: "Crop" crop_param { axis: 2} propagate_down: 1 propagate_down: 0 }
layer { bottom: 'conv2_ha' bottom: 'up_conv3' top: 'conv2_r' name: 'concat2' type: 'Concat'
  concat_param {axis: 1} }

layer { bottom: 'conv2_r' top: 'conv2_d1' name: 'conv2_d1' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 32 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }
layer { bottom: 'conv2_d1' top: 'conv2_d1' name: 'relu2_d1' type: "ReLU" }


layer {name: "conv2_subConv" type: "Convolution" bottom: "conv2_d1" top: "conv2_subConv"
  param {lr_mult: 1 decay_mult: 1} param {lr_mult: 2 decay_mult: 0} convolution_param {num_output: 128 kernel_size: 3 pad:1 weight_filler{ type: "gaussian" std: 0.01 } bias_filler{ type: "constant" value: 0} } }


layer {
  type: "Python" 
  name: "conv2_up" 
  bottom: "conv2_subConv" 
  top: "conv2_up"   
  python_param {
      module: "PyPixelShuffleLayer"
      layer: "PyPixelShuffleLayer"
      param_str: "'scale_factor': 2"
  } 
}


layer { bottom: 'conv1_2' top: 'conv1_h' name: 'conv1_h' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 32 pad: 1 kernel_size: 3  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }


layer { bottom: 'conv1_h' top: 'conv1_h' name: 'relu1_h' type: "ReLU" }


layer {
    bottom: "conv1_h"
    top: "conv1h_branch1a"
    name: "conv1h_branch1a"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv1h_branch1a"
    top: "conv1h_branch1a"
    name: "bn1_branch1a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv1h_branch1a"
    top: "conv1h_branch1a"
    name: "scale1h_branch1a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv1h_branch1a"
    top: "conv1h_branch1a"
    name: "conv1h_branch1a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv1h_branch1a"
    top: "conv1h_branch2a"
    name: "conv1h_branch2a"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false
    }
}

layer {
    bottom: "conv1h_branch2a"
    top: "conv1h_branch2a"
    name: "bn1_branch2a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv1h_branch2a"
    top: "conv1h_branch2a"
    name: "scale1h_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv1h_branch2a"
    top: "conv1h_branch2a"
    name: "conv1h_branch2a_relu"
    type: "ReLU"
}

layer {
    bottom: "conv1h_branch2a"
    top: "conv1h_branch3a"
    name: "conv1h_branch3a"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
}

layer {
    bottom: "conv1h_branch3a"
    top: "conv1h_branch3a"
    name: "bn1_branch3a"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}

layer {
    bottom: "conv1h_branch3a"
    top: "conv1h_branch3a"
    name: "scale1h_branch3a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv1_h"
    bottom: "conv1h_branch3a"
    top: "conv1_ha"
    name: "conv1_ha"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "conv1_ha"
    top: "conv1_ha"
    name: "conv1ha_relu"
    type: "ReLU"
}



layer { bottom: 'conv2_up' bottom: 'conv1_ha' top: 'up_conv2' name: 'align1' type: "Crop" crop_param { axis: 2} propagate_down: 1 propagate_down: 0 }
layer { bottom: 'conv1_h' bottom: 'up_conv2' top: 'conv1_r' name: 'concat1' type: 'Concat'
  concat_param {axis: 1} }


layer { bottom: 'conv1_r' top: 'conv1_d1' name: 'conv1_d1' type: "Convolution"
  param { lr_mult: 0.01 decay_mult: 1 } param { lr_mult: 0.02 decay_mult: 0}
  convolution_param { engine: CUDNN num_output: 1 pad: 1 kernel_size: 1  weight_filler {type: "gaussian" std: 0.01}  bias_filler {type: "constant" value:0} } }


layer { type: "Crop" name: 'crop_refine1' bottom: 'conv1_d1' bottom: 'data' top: 'upscore-final'  crop_param { axis: 2 offset: 34 offset: 34} propagate_down: 1 propagate_down: 0  }


##**Experiment of different loss functions**##

#layer {
#  name: "final_sig"
#  bottom: "upscore-final"
#  top: "result_s"
#  type: "Sigmoid"
#}


#layer {type: "SigmoidCrossEntropyLoss" bottom: "upscore-final" bottom: "label" top: "refine_loss" loss_weight: 1} 


#layer {type: "DiceCoefLoss" bottom: "result_s" bottom: "label" top:"refine_loss" loss_weight: 10}


#layer { 
#    type: 'Python' 
#    name: 'refine_loss' 
#    top: 'refine_loss' 
#    bottom: 'final_result' 
#    bottom: "label"  
#	python_param { 
	# the module name -- usually the filename -- that needs to be in $PYTHONPATH 
#		module: 'pyLayer' 
    # the layer name -- the class name in the module 
#		layer: 'DiceLoss' 
#	} 
    # set loss weight so Caffe knows this is a loss layer. 
    # since PythonLayer inherits directly from Layer, this isn't automatically 
    # known to Caffe 
#    loss_weight: 1 
#} 


#layer {
#  name: "refine_loss"
#  type: "FocalLoss"
#  bottom: "result_s"
#  bottom: "label"
#  top: "refine_loss"
#  loss_weight: 1
#  loss_param{
#    normalize: true
#    normalization: FULL
    #alpha = 0.75
    #gamma = 2
#  }
#  focal_loss_param{
#    alpha: 0.25
#    gamma: 2.0
#  } 
#}


#layer {
#  name: "refine_loss"
#  type: "FocalLoss"
#  bottom: "upscore-final"
#  bottom: "label"
#  propagate_down: 1
#  propagate_down: 0
#  top: "refine_loss"
#  include { phase: TRAIN }
#  loss_weight: 1
#  loss_param { ignore_label: -1 normalize: true }
#  focal_loss_param { alpha: 0.25 gamma: 2 }
#}


##**Experiment of different loss functions**##

layer {
  name: "refine_loss1"
  type: "ClassBalancedSigmoidCrossEntropyAttentionLossLayer"
  bottom: "upscore-final"
  bottom: "label"
  top: "refine_loss1"
  loss_weight: 0.6
  attention_loss_param {
    beta:  2
    gamma: 0.5
  }
}

layer {type: "SigmoidCrossEntropyLoss" bottom: "upscore-final" bottom: "label" top: "refine_loss" loss_weight: 0.4} 


##**End**##